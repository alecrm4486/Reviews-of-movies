{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code project: Sentiment analysis for movie reviews \n",
    "### Cecilia Rivera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from os import listdir\n",
    "import glob\n",
    "import re\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, MaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the joint all opinions, one opinion one row\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "# open the file as read only\n",
    "    file = open(filename, 'r', encoding=\"utf8\", errors='ignore')\n",
    "# read all text\n",
    "    text = file.read()\n",
    "# close the file\n",
    "    file.close()\n",
    "    return text\n",
    "#Customize sort\n",
    "pat=re.compile(\"(\\d+)\\D*$\")\n",
    "def key_func(x):\n",
    "        mat=pat.search(os.path.split(x)[-1]) # match last group of digits\n",
    "        if mat is None:\n",
    "            return x\n",
    "        return \"{:>10}\".format(mat.group(1))\n",
    "\n",
    "#clean integers\n",
    "def my_preprocessor(text):\n",
    "    text=re.sub(\"\\\\W\",\" \",text) # remove special chars\n",
    "    #text=re.sub(\"\\\\s+(in|the|all|for|and|on)\\\\s+\",\" _connector_ \",text) # normalize certain words\n",
    "    tokens=word_tokenize(text)\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    #stop_words = set(stopwords.words('english'))\n",
    "    #tokens = [w for w in tokens if not w in stop_words]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc, clean and return line of tokens\n",
    "def doc_to_line(filename):\n",
    "# load the doc\n",
    "    text = load_doc(filename)\n",
    "# clean doc\n",
    "    tokens = my_preprocessor(text)\n",
    "    return ''.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all docs in a directory\n",
    "def process_docs(directory):\n",
    "    lines = list()\n",
    "# walk through all files in the folder\n",
    "    for filename in sorted(listdir(directory), key= key_func):\n",
    "# create the full path of the file to open\n",
    "        path = directory + '/' + filename \n",
    "# load and clean the doc\n",
    "        line = doc_to_line(path)\n",
    "# add to list\n",
    "        lines.append(line)\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_list(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training/test reviews\n",
    "text_lines = process_docs('/Users/cecilia/Desktop/movie/totest/')\n",
    "save_list(text_lines, 'TTTest.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = sorted(glob.glob1(\"/Users/cecilia/Desktop/movie/totest/\", \"*\"), key = key_func)\n",
    "y=[int(re.match('.*(?:\\D|^)(\\d+)', i).group(1)) for i in A]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"testlab.txt\", \"w\") as f:\n",
    "    for i in y:\n",
    "        f.write(str(i) +\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_lines = process_docs('/Users/cecilia/Desktop/movie/totrain/')\n",
    "save_list(text_lines, 'TTTrain.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# charging the labels train\n",
    "# charging the target\n",
    "B = sorted(glob.glob1(\"/Users/cecilia/Desktop/movie/totrain/\", \"*\"), key = key_func)\n",
    "y=[int(re.match('.*(?:\\D|^)(\\d+)', i).group(1)) for i in B]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tt.txt\", \"w\") as f:\n",
    "    for i in y:\n",
    "        f.write(str(i) +\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#charging the labels\n",
    "y=[]\n",
    "with open(\"/Users/cecilia/Desktop/movie/tt.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        y.append(int(line.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(y[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#charging the corpus\n",
    "reviews=[]\n",
    "with open(\"/Users/cecilia/Desktop/movie/TTTrain.txt\") as total: \n",
    "    reviews = ([(review) for review in enumerate(total.readlines())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24999, 'i am amazed at how this movie and most others has a average 5 stars and lower when there are crappy movies averaging 7 to 10 stars on imdb the fanboy mentality strikes again when this movie came out just about everyone slammed it even my ex girlfriend said this movie questionable years later i sat down to watch this movie and i found myself enjoying even laughing quite a bit this and the replacement killers are the movies that had people labeling the director antoine fuqua as the black michael bay i don t see how since most of fuqua s movies are smarter than anything michael bay has came up with at any rate br br story alvin sanders jamie foxx is former convict that is used by a no nonsense treasury agent edgar david morse as a pawn to catch a killer named bristol doug hutchinson alvin s every moves are tracked by a bug implanted in his jaw after an accident while these agents are after bristol bristol is after the gold bricks that were taken in a heist gone awry br br jamie foxx is funny as well as great as alvin sanders alvin is a fast talker that is a lot smarter than he lets on doug hutchinson is okay as bristol he can be over the top sometimes in his john malkovitchesque demeanor he was better here than he was as looney bin jim in punisher war zone david morse is good as the hard edged treasury agent even mike epps is funny as alvin s brother stevie both him and jamie had some funny moments on screen br br the only flaw of the movie is the some of the attempts at a thriller fall flat the scenario at the horse race track is way over the top but i couldn t look away the director went all out there so he gets points for that plus the bomb scene with the treasury agent tied to a chair while the detonator rests on the door was pretty nifty br br all in all bait is not a bad movie by a long shot its never boring its always funny and i wasn t checking my watch every minute that should count for something bait is one of the most underrated movies of 2000 period br br ps to the reviewer that claimed this movie is too violent how long have you been living under a rock i m pretty sure you ve seen the die hard series and every movie by quentin tarantino but those movies aren t violent right weirdo\\n')\n"
     ]
    }
   ],
   "source": [
    "print(reviews[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assigning the labels to a vector\n",
    "y_train= [columm[1] for columm in reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assigning the corpus to a vector\n",
    "train= [columm[0] for columm in reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#working on the training dataset\n",
    "#vectorizing my corpus\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from keras.preprocessing import sequence \n",
    "# set settings and fit the count vectorizer min_df=5\n",
    "tfidf_vec=TfidfVectorizer(min_df=5, stop_words=\"english\")\n",
    "tf_idf=tfidf_vec.fit_transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the cosin similsrity \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "cosine_similarities = linear_kernel(tf_idf[0:1], tf_idf).flatten()\n",
    "related_docs_indices = cosine_similarities.argsort()[:-5:-1]\n",
    "related_docs_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make sense of the tf_idf representation\n",
    "def top_tfidf_feats(row, features, top_n=25):\n",
    "    topn_ids = np.argsort(row)[::-1][:top_n]\n",
    "    top_feats = [(features[i], row[i]) for i in topn_ids]\n",
    "    df = pd.DataFrame(top_feats)\n",
    "    df.columns = ['feature', 'tfidf']\n",
    "    return df\n",
    "\n",
    "def top_feats_in_doc(Xtr, features, row_id, top_n=25):\n",
    "  row = np.squeeze(Xtr[row_id].toarray())\n",
    "  return top_tfidf_feats(row, features, top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#showing the ten most important features in the first movie review\n",
    "top_feats_in_doc(tf_idf, features, 0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the averge of most important word of all documents\n",
    "def top_mean_feats(Xtr, features, grp_ids=None, min_tfidf=0.1, top_n=25):\n",
    "    if grp_ids:\n",
    "        D = Xtr[grp_ids].toarray()\n",
    "    else:\n",
    "        D = Xtr.toarray()\n",
    "    D[D < min_tfidf] = 0\n",
    "    tfidf_means = np.mean(D, axis=0)\n",
    "    return top_tfidf_feats(tfidf_means, features, top_n)\n",
    "\n",
    "top_mean_feats(tf_idf, features, top_n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ploting the top 15 most important features by each label\n",
    "def top_feats_by_class(Xtr, y, features, min_tfidf=0.1, top_n=25):\n",
    "    dfs = []\n",
    "    labels = np.unique(y)\n",
    "    for label in labels:\n",
    "        ids = np.where(y==label)\n",
    "        feats_df = top_mean_feats(Xtr, features, ids, min_tfidf=min_tfidf, top_n=top_n)\n",
    "        feats_df.label = label\n",
    "        dfs.append(feats_df)\n",
    "    return dfs\n",
    "\n",
    "def plot_tfidf_classfeats_h(dfs):\n",
    "    fig = plt.figure(figsize=(12, 9), facecolor=\"w\")\n",
    "    x = np.arange(len(dfs[0]))\n",
    "    for i, df in enumerate(dfs):\n",
    "        ax = fig.add_subplot(1, len(dfs), i+1)\n",
    "        ax.spines[\"top\"].set_visible(False)\n",
    "        ax.spines[\"right\"].set_visible(False)\n",
    "        ax.set_frame_on(False)\n",
    "        ax.get_xaxis().tick_bottom()\n",
    "        ax.get_yaxis().tick_left()\n",
    "        ax.set_xlabel(\"Mean Tf-Idf Score\", labelpad=10, fontsize=10)\n",
    "        ax.set_title(\"label = \" + str(df.label), fontsize=10)\n",
    "        ax.ticklabel_format(axis='x', style='sci', scilimits=(-2,2))\n",
    "        ax.barh(x, df.tfidf, align='center', color='#3F5D7D')\n",
    "        ax.set_yticks(x)\n",
    "        ax.set_ylim([-1, x[-1]+1])\n",
    "        yticks = ax.set_yticklabels(df.feature)\n",
    "        plt.subplots_adjust(bottom=0.09, right=0.97, left=0.15, top=0.95, wspace=0.52)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tfidf_classfeats_h(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the CNN model\n",
    "#the training data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train)\n",
    "sequences = tokenizer.texts_to_matrix(train, mode='tfidf')\n",
    "word_index = tokenizer.word_index\n",
    "x_train = pad_sequences(sequences, maxlen=25000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the labels\n",
    "# prepare target\n",
    "def prepare_targets(y_train):\n",
    "    le = LabelEncoder()\n",
    "    le.fit(y_train)\n",
    "    y_train_enc = le.transform(y_train)\n",
    "    return y_train_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y= prepare_targets(y_train)\n",
    "labels= to_categorical(y, num_classes=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the enbedding layer and loading the pre-trained weights\n",
    "#using pre traned data\n",
    "import numpy as np\n",
    "#preparing the embedding layer\n",
    "import os\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(\"/content/drive/My Drive/\", 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a word_index to compute the matrice embeddings\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 100))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN architecture\n",
    "myclass= Sequential()\n",
    "myclass.add(Embedding(len(word_index) + 1,100, weights=[embedding_matrix], input_length=25000, trainable=False))\n",
    "myclass.add(Conv1D(100, kernel_size=5, activation='relu'))\n",
    "myclass.add(MaxPooling1D(5))\n",
    "myclass.add(Conv1D(100, kernel_size=5, activation='relu'))\n",
    "myclass.add(MaxPooling1D(5))\n",
    "myclass.add(Conv1D(100, kernel_size=5, activation='relu')) \n",
    "myclass.add(MaxPooling1D(25))\n",
    "myclass.add(Flatten())\n",
    "myclass.add(Dense(units=100, activation='relu'))\n",
    "myclass.add(Dense(8, activation='softmax'))\n",
    "myclass.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myclass.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MModel=myclass.fit(x_train, labels, epochs=5, batch_size=128,validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot of accuracy and loss for validation\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(MModel.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(MModel.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot of accuracy and loss for training\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(MModel.history['acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(MModel.history['loss'])\n",
    "#plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the model\n",
    "myclass.save('/content/drive/My Drive/mylast.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "#the second part of the project test the trained model\n",
    "from keras.models import load_model\n",
    "mymodel = load_model('/content/drive/My Drive/mylast.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#charging the labels\n",
    "y1=[]\n",
    "with open(\"/content/drive/My Drive/testlab.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        y1.append(int(line.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#charging the corpus\n",
    "opinions=[]\n",
    "with open(\"/content/drive/My Drive/TTTest.txt\") as totale: \n",
    "    opinions = ([(opinion, y1[k]) for k,opinion in enumerate(totale.readlines())])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing the labels\n",
    "def prepare_targets(y_train):\n",
    "    le = LabelEncoder()\n",
    "    le.fit(y_train)\n",
    "    y_train_enc = le.transform(y_train)\n",
    "    return y_train_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y= prepare_targets(y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing the corpus\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "token = Tokenizer()\n",
    "token.fit_on_texts(x_t)\n",
    "seq = token.texts_to_matrix(x_t, mode='tfidf')\n",
    "x_test = pad_sequences(seq, maxlen=25000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make the predictions\n",
    "classes = mymodel.predict_classes(x_test, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating the confussion matrix and plot\n",
    "from sklearn.metrics import confusion_matrix\n",
    "con_mat = confusion_matrix(y, classes)\n",
    "\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "df_cm = pd.DataFrame(con_mat, range(8), range(8))\n",
    "sn.set(font_scale=1.2) # for label size\n",
    "sn.heatmap(con_mat, annot=True, annot_kws={\"size\": 12}, fmt=\"d\") # font size\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating the precision and recall\n",
    "def precision(label, confusion_matrix):\n",
    "    col = confusion_matrix[:,label]\n",
    "    return confusion_matrix[label, label] / col.sum()\n",
    "    \n",
    "def recall(label, confusion_matrix):\n",
    "    row = confusion_matrix[label, :]\n",
    "    return confusion_matrix[label, label] / row.sum()\n",
    "\n",
    "def precision_macro_average(confusion_matrix):\n",
    "    rows, columns = confusion_matrix.shape\n",
    "    sum_of_precisions = 0\n",
    "    for label in range(rows):\n",
    "        sum_of_precisions += precision(label, confusion_matrix)\n",
    "    return sum_of_precisions / rows\n",
    "\n",
    "def recall_macro_average(confusion_matrix):\n",
    "    rows, columns = confusion_matrix.shape\n",
    "    sum_of_recalls = 0\n",
    "    for label in range(columns):\n",
    "        sum_of_recalls += recall(label, confusion_matrix)\n",
    "    return sum_of_recalls / columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"label precision recall\")\n",
    "for y in range(8):\n",
    "    print(f\"{y:5d} {precision(y, con_mat):9.3f} {recall(y, con_mat):6.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"recall total:\", recall_macro_average(con_mat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating the accuracy level\n",
    "def accuracy(confusion_matrix):\n",
    "    diagonal_sum = confusion_matrix.trace()\n",
    "    sum_of_all_elements = confusion_matrix.sum()\n",
    "    return diagonal_sum / sum_of_all_elements "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
